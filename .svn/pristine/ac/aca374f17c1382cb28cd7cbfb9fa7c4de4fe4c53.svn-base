Paper scope and motivation
---------------------------

(i) Differentiated partitioning (Rev 1)

Application SLA guided cache sizing solutions like SDC [30] and Centaur
establish a relation between SLA requirements and cache size.
Cache manager's knowledge of applications and a clear isolation 
between them is the underlying assumption of the above techniques. 
In a nested setup like derivative cloud, the above mentioned
techniques can not be applied directly without the SLA of individual
applications (containers) are managed at the hypervisor resource management
level. This becomes a contradiction to the derivative cloud framework
where the VM provider (e.g., EC2) does not need to know the application 
semantic and need not adhere to application SLAs to manage its resource across VMs. 
DoubleDecker bridges this gap by delegating the responsibilities of application 
SLA adherence to the VM resource manager and provide a mechanism to translate
these SLAs into priorities before enlightening the hypervisor resource manager
with the same. We believe our framework is generic and orthogonal to application SLA 
guided cache management solutions and enables holistic disk cache management in a 
nested derivative cloud setup. 

(ii) SLA guided management policy (All reviewers)

We believe DoubleDecker provides enough flexibility to design automated
SLA adhering policies at the container level. The VM provider's changing
commitment in terms of hypervisor cache allocation to VMs can be absorbed
by changing the intra-VM container weights and target caching devices. 
Moreover, employing techniques like SDC[30] and Centaur from within
the VM to manage caching resource across containers is practically feasible
because of the proximity between the guest OS and containers. 

(iii) Motivation (Rev 2)

We agree with the reviewer that fair cache usage may not provide best results in 
a holistic sense. But if a high level policy (like [30]) translates into a fair
allocation (or allocation with some weights) the hypervisor cache distribution 
should adhere to that. This is exactly what is missing in the existing solutions
and enabled by DoubleDecker.
 
Design
------
(i) Alternates to weights? (Reviewer 1)

In the derivative cloud setup, any application within a VM can not (and should not) 
dictate an absolute limit because it depends on dynamic nature of available resources and 
competing VMs, at the system level. However, depending on cache availability, the VM level 
admin can set priorities to get required cache share. The reviewers points regarding 
cgroup limits (memory etc.) are not mandatorily adhered to in a resource crunch scenario.

(ii) DoubleDecker requires change in many components (All reviewers)

We agree, but we did not come across any full proof black box 
solution to differentially treat intra-VM applications.

(iii) Optimizations (deduplication, compression)

Not the primary focus of the paper, already provided by other 
hypervisor cache solutions like Xen tmem, KVM zcache.


Experiments
-----------
(i) Why not multiple containers and multiple VMs in the same 
experiment? (Rev 1 and 3)

Dynamically changing the weights at VM and container level, 
bringing in new VMs and Containers in the same experiment 
becomes a complicated explanation. Therefore, we have split these
two aspects into two different experiments to show the dynamic 
adaptability of DoubleDecker framework.

(ii) Table 1 and Figure 10 results 

Two different experiments performed with different workload 
setting (total working set), container memory limit configurations and 
hypervisor cache size limit. Hence the difference in results.

(iii) Peak usage of workloads

SSD backed hypervisor cache usage by workloads (Fig 8(c) and Fig 9) show
the peak usage of workloads.   
